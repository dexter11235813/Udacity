library(MASS)
summary(mamamals)
summary(mammals)
mammals$body
mammals$brain
plot(log(mammals))
boxplot(mammals)
boxplot(log(mammals),xlab='body',ylab='brain')
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr", repos = "http://mran.revolutionanalytics.com")
library(devtools)
install.packages(devtools)
url <- "http://www.rdatamining.com/data/rdmTweets.RData"
download.file(url, destfile = "./data/rdmTweets.RData")
install.packages("ROAuth")
install.packages("twitteR")
ilbrary("ROAuth")
library("ROAuth")
install.packages("twitteR")
install.packages("twitteR")
library("twitteR")
setup_twitter_oauth()
consumer_key=PKIqzBYSv4TRN4SnR8MXnW1kQ
consumer_key="PKIqzBYSv4TRN4SnR8MXnW1kQ"
consumer_secret="ZRWS4RXUDDe4o3FWArxUofthP7zjnUa3lv7gzwRSYQMqtWkJi9
"
setup_twitter_oauth(consumer_key, consumer_secret)
install.packages("base64enc")
setup_twitter_oauth(consumer_key, consumer_secret)
load("~/Desktop/Math 289/.RData")
head(train)
ntree = 100; numCore = 4
rep <- ntree/numCore # tree / numCore
registerDoParallel(cores=numCore)
begTime <- Sys.time()
random.forest <- foreach(ntree=rep(rep, numCore), .combine=combine,
.packages="randomForest") %dopar%
randomForest(as.factor(target) ~ . , data = train_train,ntree = ntree,mtry = 10,importance = T,replace = F)
#rf = randomForest(as.factor(target) ~ .,data = train_train )
pred = predict(random.forest,train_test)
length(which(pred == labels))/dim(train_test)[1]
runTime <- Sys.time()-begTime
runTime
library(doParallel)
library(zoo)
library(randomForest)
library(e1071)
library(parallelSVM)
####
fillNA <- function(S)
{
L <- !is.na(S)
c(S[L][1], S[L])[cumsum(L)+1]
}
train = read.csv("train.csv", header = TRUE)
train1 = train
fac = which(lapply(train,class)=="factor")
factor = train[,fac]
factor = factor[,-c(2,8,19)]
train = train[,-fac]
train = train[,-1]
train = data.frame(lapply(train,fillNA))
# add back the factor terms
dim(train)
train1 = train
fac = which(lapply(train,class)=="factor")
factor = train[,fac]
factor = factor[,-c(2,8,19)]
train = train[,-fac]
train = train[,-1]
train = data.frame(lapply(train,fillNA))
# add back the factor terms
train = cbind(train,factor)
dim(train)
load("~/Desktop/Math 289/.RData")
library(doParallel)
library(zoo)
library(randomForest)
library(e1071)
library(parallelSVM)
dim(factor)
dim(train)
ntree = 100; numCore = 4
rep <- ntree/numCore # tree / numCore
registerDoParallel(cores=numCore)
begTime <- Sys.time()
random.forest <- foreach(ntree=rep(rep, numCore), .combine=combine,
.packages="randomForest") %dopar%
randomForest(as.factor(target) ~ . , data = train_train,ntree = ntree,mtry = 10,importance = T,replace = F)
#rf = randomForest(as.factor(target) ~ .,data = train_train )
pred = predict(random.forest,train_test)
length(which(pred == labels))/dim(train_test)[1]
runTime <- Sys.time()-begTime
runTime
View(dummy)
rm(list =ls())
install.packages(c('rzmq','repr','IRkernel','IRdisplay'), repos = 'http://irkernel.github.io/', type = 'source')
IRkernel::installspec()
quit()
so  = [2 3 5]
so  = c(2,4,7)
sum = exp(so)
sum
sum = sum(sum)
sum
sum - so[3]
sum + so[3]
so = c(2,3,5)
su =  exp(so)
rm(sum)
su
s = sum(su)
s
s-su[23]
s-su[3]
s-su[3]-su[2]
load("~/Desktop/Math 289/.RData")
setwd("/home/abhijit331/Desktop/Math 289")
head(spar)
random.forest
random.forest$importance
plot(random.forest$importance)
varimpPlot(random.forest)
library(randomForest)
varimpPlot(random.forest)
varImpPlot(random.forest)
names(random.forest$importance)
random.forest$importance[1]
random.forest$importance[1,]
random.forest$importance[1,4]
good.features = which(random.forest$importance[,4] > 200)
good.features
good.features = random.forest$importance[which(random.forest$importance[,4]) > 200,]
good.features = random.forest$importance[which(random.forest$importance[,4] > 200),]
good.features
good.features = random.forest$importance[which(random.forest$importance[,4] > 100),]
good.features
good.features = random.forest$importance[which(random.forest$importance[,3] > 0.05),]
good.features
random.forest$importance[,3]
which(random.forest$importance[,3] > 0.005)
which(random.forest$importance[,3] > 0.005)
which(random.forest$importance[,3] > 0.003)
which(random.forest$importance[,3] > 0.001)
which(random.forest$importance[,3] > 0.002)
good.features.accuracy = random.forest$importance[which(random.forest$importance[,3] > 0.002),]
good.features.accuracy
dim(good.features.accuracy)
good.features.accuracy = random.forest$importance[which(random.forest$importance[,3] > 0.001),]
good.features.accuracy = random.forest$importance[which(random.forest$importance[,3] > 0.0025),]
dim(good.features.accuracy)
good.features.accuracy = random.forest$importance[which(random.forest$importance[,3] > 0.0015),]
dim(good.features.accuracy)
good.features.accuracy
names(good.features.accuracy)
which(random.forest$importance[,3] > 0.0015)
names(dum)
head(train)
names(train)
counts = c()
for(i in 1:dim(train1)[1])
{
counts = c(counts,length(which(is.na(train1[i,]) == TRUE)))
}
counts = c()
for(i in 1:dim(train1)[1])
{
counts = c(counts,length(which(is.na(train1[i,]) == TRUE)))
}
length(counts)
counts = c()
for(i in 1:dim(train1)[1])
{
counts = c(counts,length(which(is.na(train1[i,]) == TRUE)))
}
train = cbind(train,counts)
names(train)
head(train$counts)
# Extreme Gradient Boosting
dum = train[1:100000,-1]
la = train[1:100000,1]
dumtest = train[100001:110000,-1]
spar2 = sparse.model.matrix(~.,data = dumtest)
latest = train[100001:110000,1]
spar = sparse.model.matrix(~.,data = dum)
xgb.model = xgboost(data = spar,label =as.factor(la),max_depth = 9,eta = 1 , nthread = 4,nround = 10,onjective = "binary:logistic")
impo =xgb.importance(feature_names = spar@Dimnames[[2]],model = xgb.model)
testp = predict(xgb.model,newdata = spar2)
testp=ifelse(testp > 0.5,1,0)
length(which(testp == latest))
library(doParallel)
library(zoo)
library(xgboost)
library(MASS)
library(randomForest)
library(e1071)
library(parallelSVM)
dum = train[1:100000,-1]
la = train[1:100000,1]
dumtest = train[100001:110000,-1]
spar2 = sparse.model.matrix(~.,data = dumtest)
latest = train[100001:110000,1]
spar = sparse.model.matrix(~.,data = dum)
xgb.model = xgboost(data = spar,label =as.factor(la),max_depth = 9,eta = 1 , nthread = 4,nround = 10,onjective = "binary:logistic")
impo =xgb.importance(feature_names = spar@Dimnames[[2]],model = xgb.model)
testp = predict(xgb.model,newdata = spar2)
testp=ifelse(testp > 0.5,1,0)
length(which(testp == latest))
impo =xgb.importance(feature_names = spar@Dimnames[[2]],model = xgb.model)
impo
detectCoreS()
detectCores()
?detectCores()
setwd("/home/abhijit331/Desktop/Udacity")
data = read.csv("titanic_data.csv",header = T)
head(data)
data = data[,-2]
head(data)
dim(head)
dim(data)
rm(data)
